{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df7f24c0"
      },
      "source": [
        "# Task\n",
        "Perform a comprehensive analysis of the 'blood-transfusion-service-center' dataset from \"https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\". This includes loading and preparing the data, handling missing values and duplicates, conducting exploratory data analysis with visualizations (histograms, box plots, correlation heatmap), preparing the data for machine learning by encoding the target variable and splitting into training/testing sets, building and training a simple classification model, and finally, evaluating its performance using appropriate metrics. The final deliverable should summarize insights from cleaning, analysis, visualizations, and model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50e84a13"
      },
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Download the 'blood-transfusion-service-center' dataset from the provided URL, convert it into a pandas DataFrame, and perform an initial inspection of its structure, data types, and first few rows. I will use the dataset description to understand the meaning of each column (V1: Recency, V2: Frequency, V3: Monetary, V4: Time, and the target variable).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6feaa274"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the pandas library, define the dataset URL, load the dataset into a DataFrame, and then display the first 5 rows, column information, and descriptive statistics as requested in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a95c0b43"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the URL for the dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data'\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "# The dataset has a header, so we'll let pandas infer it or explicitly set header=0\n",
        "# The separator is ',', and '?' is specified as a missing value indicator, although inspecting the data suggests no '?' are present.\n",
        "# For safety and clarity, let's explicitly name the columns as per the problem description.\n",
        "column_names = ['Recency (months)', 'Frequency (times)', 'Monetary (c.c. blood)', 'Time (months)', 'donated_blood_in_march']\n",
        "df = pd.read_csv(url, sep=',', na_values='?', names=column_names, header=0)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display the column names and their data types\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()\n",
        "\n",
        "# Display basic descriptive statistics of the DataFrame\n",
        "print(\"\\nDescriptive statistics of the DataFrame:\")\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aa6688c"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "### Subtask:\n",
        "Check for and handle any missing values or duplicate entries within the dataset. Ensure that all columns have appropriate data types for analysis and model building, converting them if necessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb27e7f4"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires checking for missing values and duplicate entries, and then handling duplicates if found. After these operations, it's good practice to re-verify the data types and overall DataFrame structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21d88822"
      },
      "source": [
        "print(\"\\nMissing values before cleaning:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nNumber of duplicate rows before cleaning:\")\n",
        "duplicate_rows_count = df.duplicated().sum()\n",
        "print(duplicate_rows_count)\n",
        "\n",
        "if duplicate_rows_count > 0:\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    print(f\"\\n{duplicate_rows_count} duplicate rows removed.\")\n",
        "else:\n",
        "    print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "print(\"\\nDataFrame Info after cleaning (if any duplicates were found and removed):\")\n",
        "df.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c964f834"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "### Subtask:\n",
        "Conduct a detailed exploratory data analysis. This includes generating descriptive statistics for all features, analyzing the distribution of numerical features, and investigating the relationships between the features and the target variable (whether a donor donated blood in March 2007).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad52c05f"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-examine the basic descriptive statistics for the cleaned DataFrame to understand the central tendency, dispersion, and shape of the dataset's distribution after removing duplicates. This addresses the first instruction of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ce70233"
      },
      "source": [
        "print(\"\\nDescriptive statistics of the DataFrame after duplicate removal:\")\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286264ed"
      },
      "source": [
        "**Reasoning**:\n",
        "To analyze the distribution of numerical features and check for skewness or potential outliers, I will generate histograms for each numerical column. This addresses the second instruction of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94bee0d2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the aesthetic style of the plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# List of numerical features to plot\n",
        "numerical_features = ['Recency (months)', 'Frequency (times)', 'Monetary (c.c. blood)', 'Time (months)']\n",
        "\n",
        "# Create histograms for each numerical feature\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_features):\n",
        "    plt.subplot(2, 2, i + 1) # Arrange in 2 rows, 2 columns\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45a4a813"
      },
      "source": [
        "**Reasoning**:\n",
        "To investigate the relationship between each numerical feature and the target variable, I will calculate the mean of each numerical feature grouped by the 'donated_blood_in_march' target variable. This will help identify noticeable differences between donors and non-donors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be63ddaa"
      },
      "source": [
        "print(\"\\nMean of numerical features grouped by 'donated_blood_in_march':\")\n",
        "print(df.groupby('donated_blood_in_march')[numerical_features].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac99f5c1"
      },
      "source": [
        "**Reasoning**:\n",
        "To visually investigate the relationship between each numerical feature and the target variable, I will generate box plots for each numerical feature, colored by 'donated_blood_in_march'. This will complement the mean comparison and help identify differences in distributions between donors and non-donors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b4aa9f0"
      },
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_features):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    sns.boxplot(x='donated_blood_in_march', y=col, data=df)\n",
        "    plt.title(f'{col} by Donated Blood in March')\n",
        "    plt.xlabel('Donated Blood in March (0: No, 1: Yes)')\n",
        "    plt.ylabel(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67bab8c"
      },
      "source": [
        "## Visualize Cleaned and Analyzed Data\n",
        "\n",
        "### Subtask:\n",
        "Create various visualizations to clearly represent the cleaned data and the insights gained from the EDA. This will include plots like histograms for feature distributions, box plots to identify outliers, and potentially a correlation heatmap to show relationships between variables, including the target.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7e4e466"
      },
      "source": [
        "**Reasoning**:\n",
        "To investigate the linear relationships between all numerical features and the target variable, I will calculate the correlation matrix and visualize it using a heatmap with annotations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a96389f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate the correlation matrix for all relevant columns, including the target\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f9b2eca"
      },
      "source": [
        "## Prepare Data for Machine Learning\n",
        "\n",
        "### Subtask:\n",
        "Separate the dataset into features (X) and the target variable (y). The target variable 'donated blood in March 2007' will be encoded (e.g., 2 to 1 for donation, 1 to 0 for no donation). The data will then be split into training and testing sets to prepare for model building.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7546303"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare the data for machine learning, I will first separate the features (X) from the target variable (y), then split the data into training and testing sets for model evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ec576da"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('donated_blood_in_march', axis=1)\n",
        "y = df['donated_blood_in_march']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb4b4c8a"
      },
      "source": [
        "## Build and Train a Simple Classification Model\n",
        "\n",
        "### Subtask:\n",
        "Implement and train a simple classification model, such as Logistic Regression or a Decision Tree Classifier, to predict the target variable based on the prepared features. The model's objective will be to classify whether a donor will donate blood.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "273273df"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement and train a simple classification model, I will import `LogisticRegression`, instantiate it, and then train it using the `fit` method on the prepared training data (`X_train`, `y_train`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "772ec658"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, solver='liblinear') # Using 'liblinear' solver for small datasets\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37c8ce4f"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the model has been trained, the next step is to evaluate its performance. To do this, I will use the trained model to make predictions on the test set (`X_test`) and store these predictions. This is a crucial step before calculating evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894ec4e3"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(\"Model predictions on the test set generated successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a55447f4"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the performance of the trained model, I will calculate common classification metrics such as accuracy, precision, recall, and F1-score, and also generate a confusion matrix. This will provide a comprehensive understanding of the model's performance on the unseen test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32171bf5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nModel Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"\\nConfusion Matrix:\\n{conf_matrix}\")\n",
        "\n",
        "# Optionally visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ed1175"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a comprehensive summary of the entire process, including key insights from data cleaning and analysis, the visualizations created, and the performance metrics of the simple machine learning model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc156e98"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial dataset, comprising 748 entries and 5 columns, was successfully loaded and inspected. All features were numerical (`int64`), and the target variable, 'donated\\_blood\\_in\\_march', indicated an imbalanced dataset where approximately 23.8% of entries represented donors.\n",
        "*   Data cleaning revealed no missing values. However, 215 duplicate rows were identified and removed, reducing the dataset size to 533 unique entries.\n",
        "*   Exploratory Data Analysis (EDA) showed that all numerical features ('Recency (months)', 'Frequency (times)', 'Monetary (c.c. blood)', 'Time (months)') exhibited right-skewed distributions.\n",
        "*   Analysis of feature means grouped by the target variable provided key insights into donor behavior:\n",
        "    *   Donors who gave blood in March had a significantly lower mean 'Recency' (5.89 months) compared to non-donors (10.94 months).\n",
        "    *   Donors showed a higher mean 'Frequency' (8.74 times) and 'Monetary' contribution (2186 c.c.) than non-donors (6.31 times and 1577 c.c. respectively).\n",
        "    *   Donors had a slightly lower mean 'Time' active (37.29 months) than non-donors (44.25 months).\n",
        "*   A correlation heatmap was generated, visualizing the linear relationships between all features and the target variable.\n",
        "*   The data was successfully prepared for machine learning, with features and target separated, and then split into 80% training (426 samples) and 20% testing (107 samples) sets.\n",
        "*   A Logistic Regression model was trained and evaluated, yielding the following performance metrics on the test set:\n",
        "    *   Accuracy: 0.7664\n",
        "    *   Precision: 0.5833\n",
        "    *   Recall: 0.2593\n",
        "    *   F1-Score: 0.3590\n",
        "*   The confusion matrix indicated 7 True Positives (correctly predicted donors), 20 False Negatives (actual donors missed), 5 False Positives (non-donors incorrectly predicted as donors), and 75 True Negatives (correctly predicted non-donors).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The low recall of the Logistic Regression model (0.2593) indicates a significant challenge in correctly identifying actual blood donors, which is a critical aspect for a donor prediction service. This suggests that the model is biased towards predicting the majority class (non-donors), likely due to the imbalanced nature of the dataset.\n",
        "*   To improve the model's ability to predict donors, future steps should focus on addressing the class imbalance using techniques like oversampling (e.g., SMOTE), undersampling, or using algorithms inherently robust to imbalance. Additionally, exploring feature engineering (e.g., transformations for skewed data) and more advanced classification models (e.g., Gradient Boosting, Random Forests) or hyperparameter tuning could enhance predictive performance, especially recall and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ffa1c5b"
      },
      "source": [
        "# Task\n",
        "Build a Logistic Regression model to predict a target variable using a user-provided CSV dataset. This involves loading the dataset, handling missing values and duplicates, performing exploratory data analysis (EDA) with visualizations, preprocessing data types, splitting data into training and testing sets, training the model, and evaluating its performance."
      ]
    }
  ]
}